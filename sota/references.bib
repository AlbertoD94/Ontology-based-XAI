@inproceedings{al2006link,
abstract = {Lee D D, Seung H S. Algorithms for non-negative matrix factorization[C]//Advances in neural information processing systems. 2001: 556-562.},
author = {Hasan, Mohammad Al and Chaoji, Vineet and Salem, Saeed and Zaki, Mohammed and York, New},
booktitle = {SDM06: workshop on link analysis, counter-terrorism and security},
pages = {798--805},
title = {{Link Prediction using Supervised Learning}},
volume = {30},
year = {2006}
}
@article{kang2017prediction,
abstract = {In recent years, various studies have been conducted on the prediction of crime occurrences. This predictive capability is intended to assist in crime prevention by facilitating effective implementation of police patrols. Previous studies have used data from multiple domains such as demographics, economics, and education. Their prediction models treat data from different domains equally. These methods have problems in crime occurrence prediction, such as difficulty in discovering highly nonlinear relationships, redundancies, and dependencies between multiple datasets. In order to enhance crime prediction models, we consider environmental context information, such as broken windows theory and crime prevention through environmental design. In this paper, we propose a feature-level data fusion method with environmental context based on a deep neural network (DNN). Our dataset consists of data collected from various online databases of crime statistics, demographic and meteorological data, and images in Chicago, Illinois. Prior to generating training data, we select crime-related data by conducting statistical analyses. Finally, we train our DNN, which consists of the following four kinds of layers: spatial, temporal, environmental context, and joint feature representation layers. Coupled with crucial data extracted from various domains, our fusion DNN is a product of an efficient decision-making process that statistically analyzes data redundancy. Experimental performance results show that our DNN model is more accurate in predicting crime occurrence than other prediction models.},
author = {Kang, Hyeon Woo and Kang, Hang Bong},
doi = {10.1371/journal.pone.0176244},
issn = {19326203},
journal = {PLoS ONE},
number = {4},
pages = {e0176244},
pmid = {28437486},
publisher = {Public Library of Science San Francisco, CA USA},
title = {{Prediction of crime occurrence from multimodal data using deep learning}},
volume = {12},
year = {2017}
}
@article{lee2019explainable,
abstract = {Owing to improvements in image recognition via deep learning, machine-learning algorithms could eventually be applied to automated medical diagnoses that can guide clinical decision-making. However, these algorithms remain a â€˜black box' in terms of how they generate the predictions from the input data. Also, high-performance deep learning requires large, high-quality training datasets. Here, we report the development of an understandable deep-learning system that detects acute intracranial haemorrhage (ICH) and classifies five ICH subtypes from unenhanced head computed-tomography scans. By using a dataset of only 904 cases for algorithm training, the system achieved a performance similar to that of expert radiologists in two independent test datasets containing 200 cases (sensitivity of 98{\%} and specificity of 95{\%}) and 196 cases (sensitivity of 92{\%} and specificity of 95{\%}). The system includes an attention map and a prediction basis retrieved from training data to enhance explainability, and an iterative process that mimics the workflow of radiologists. Our approach to algorithm development can facilitate the development of deep-learning systems for a variety of clinical applications and accelerate their adoption into clinical practice.},
annote = {Clinical XAI - Check},
author = {Lee, Hyunkwang and Yune, Sehyo and Mansouri, Mohammad and Kim, Myeongchan and Tajmir, Shahein H. and Guerrier, Claude E. and Ebert, Sarah A. and Pomerantz, Stuart R. and Romero, Javier M. and Kamalian, Shahmir and Gonzalez, Ramon G. and Lev, Michael H. and Do, Synho},
doi = {10.1038/s41551-018-0324-9},
issn = {2157846X},
journal = {Nature Biomedical Engineering},
number = {3},
pages = {173--182},
pmid = {30948806},
publisher = {Nature Publishing Group},
title = {{An explainable deep-learning algorithm for the detection of acute intracranial haemorrhage from small datasets}},
volume = {3},
year = {2019}
}
@article{singh2020explainable,
abstract = {Deep learning methods have been very effective for a variety of medical diagnostic tasks and has even beaten human experts on some of those. However, the black-box nature of the algorithms has restricted clinical use. Recent explainability studies aim to show the features that influence the decision of a model the most. The majority of literature reviews of this area have focused on taxonomy, ethics, and the need for explanations. A review of the current applications of explainable deep learning for different medical imaging tasks is presented here. The various approaches, challenges for clinical deployment, and the areas requiring further research are discussed here from a practical standpoint of a deep learning researcher designing a system for the clinical end-users.},
archivePrefix = {arXiv},
arxivId = {2005.13799},
author = {Singh, Amitojdeep and Sengupta, Sourya and Lakshminarayanan, Vasudevan},
eprint = {2005.13799},
issn = {23318422},
journal = {arXiv},
keywords = {Deep learning,Diagnosis,Explainability,Explainable AI,Medical imaging,XAI},
title = {{Explainable deep learning models in medical image analysis}},
year = {2020}
}
@inproceedings{stano2020explainable,
abstract = {{\ldots} Authors of [2] divide the needs of an explainable AI into four categories {\ldots} Several methods for explainability and interpretability of deep neural networks have already been {\ldots} Twelfth International Conference on Machine Vision (ICMV 2019), edited by Wolfgang Osten, Dmitry {\ldots}},
author = {Stano, Martin and Benesova, Wanda and Martak, Lukas S.},
booktitle = {Twelfth International Conference on Machine Vision (ICMV 2019)},
doi = {10.1117/12.2557314},
isbn = {9781510636439},
issn = {1996756X},
organization = {International Society for Optics and Photonics},
pages = {43},
title = {{Explainable 3D convolutional neural network using GMM encoding}},
volume = {11433},
year = {2020}
}
@article{lipton2018mythos,
archivePrefix = {arXiv},
arxivId = {1606.03490},
author = {Lipton, Zachary C.},
doi = {10.1145/3233231},
eprint = {1606.03490},
issn = {15577317},
journal = {Communications of the ACM},
number = {10},
pages = {35--43},
publisher = {ACM New York, NY, USA},
title = {{The mythos of model interpretability}},
volume = {61},
year = {2018}
}
@article{malgieri2017right,
abstract = {The aim of this contribution is to analyse the real borderlines of the 'right to explanation' in the GDPR and to discretely distinguish between dif ferent levels of information and of consumers' awareness in the 'black box society. In order to combine transparency and comprehensibility we propose the new concept of algorithm 'legibility'. We argue that a systemic interpretation is needed in this field, since it can be beneficial not only for individuals but also for businesses. This may be an opportunity for auditing algorithms and correcting unknown machine biases, thus similarly enhancing the quality of decision-making outputs. Accordingly, we show how a systemic interpretation of Articles 13-15 and 22 GDPR is necessary, considering in particular that: The threshold of minimum human intervention required so that the decision-making is 'solely' automated (Article 22(1)) can also include nominal human intervention; the envisaged 'significant effects' on individuals (Article 22(1)) can encompass as well marketing manipulation, price discrimination, etc; 'meaningful information' that should be pro-vided to data subjects about the logic, signifi-cance and consequences of decision-making (Article 15(1 )(h){\textgreater} should be read as 'legibility' of 'architecture' and 'implementation' of algorith-mic processing; trade secret protection might limit the right of access of data subjects, but there is a general legal favour for data protection rights that should reduce the impact of trade secrets protection. In addition, we recommend a 'legibility test' that data controllers should perform in order to com-ply with the duty to provide meaningful information about the logic involved in an automated decision-making.},
annote = {General Data Protection Regulation justification.},
author = {Malgieri, Gianclaudio and Comand{\'{e}}, Giovanni},
doi = {10.1093/idpl/ipx019},
issn = {2044-3994},
journal = {International Data Privacy Law},
month = {nov},
number = {4},
pages = {243--265},
title = {{Why a Right to Legibility of Automated Decision-Making Exists in the General Data Protection Regulation}},
url = {http://academic.oup.com/idpl/article/7/4/243/4626991},
volume = {7},
year = {2017}
}
@article{brynjolfsson2017can,
annote = {General XAI - Check},
author = {Brynjolfsson, Erik and Mitchell, Tom},
doi = {10.1126/science.aap8062},
issn = {10959203},
journal = {Science},
number = {6370},
pages = {1530--1534},
pmid = {29269459},
publisher = {American Association for the Advancement of Science},
title = {{What can machine learning do? Workforce implications: Profound change is coming, but roles for humans remain}},
volume = {358},
year = {2017}
}
@article{doran2017does,
abstract = {We characterize three notions of explainable AI that cut across research fields: opaque systems that offer no insight into its algorithmic mechanisms; interpretable systems where users can mathematically analyze its algorithmic mechanisms; and comprehensible systems that emit symbols enabling user-driven explanations of how a conclusion is reached. The paper is motivated by a corpus analysis of NIPS, ACL, COGSCI, and ICCV/ECCV paper titles showing differences in how work on explainable AI is positioned in various fields. We close by introducing a fourth notion: truly explainable systems, where automated reasoning is central to output crafted explanations without requiring human post processing as final step of the generative process.},
annote = {General XAI - Check},
archivePrefix = {arXiv},
arxivId = {1710.00794},
author = {Doran, Derek and Schulz, Sarah and Besold, Tarek R.},
eprint = {1710.00794},
issn = {23318422},
journal = {arXiv},
title = {{What does explainable ai really mean? A new conceptualization of perspectives}},
year = {2017}
}
@article{das2020opportunities,
abstract = {Nowadays, deep neural networks are widely used in mission critical systems such as healthcare, self-driving vehicles, and military which have direct impact on human lives. However, the black-box nature of deep neural networks challenges its use in mission critical applications, raising ethical and judicial concerns inducing lack of trust. Explainable Artificial Intelligence (XAI) is a field of Artificial Intelligence (AI) that promotes a set of tools, techniques, and algorithms that can generate high-quality interpretable, intuitive, human-understandable explanations of AI decisions. In addition to providing a holistic view of the current XAI landscape in deep learning, this paper provides mathematical summaries of seminal work. We start by proposing a taxonomy and categorizing the XAI techniques based on their scope of explanations, methodology behind the algorithms, and explanation level or usage which helps build trustworthy, interpretable, and self-explanatory deep learning models. We then describe the main principles used in XAI research and present the historical timeline for landmark studies in XAI from 2007 to 2020. After explaining each category of algorithms and approaches in detail, we then evaluate the explanation maps generated by eight XAI algorithms on image data, discuss the limitations of this approach, and provide potential future directions to improve XAI evaluation.},
annote = {General XAI - Check},
archivePrefix = {arXiv},
arxivId = {2006.11371},
author = {Das, Arun and Rad, Paul},
eprint = {2006.11371},
issn = {23318422},
journal = {arXiv},
keywords = {Computer vision,Explainable ai,Interpretable deep learning,Machine learning,Neural network,Xai},
title = {{Opportunities and Challenges in Explainable Artificial Intelligence (XAI): A Survey}},
year = {2020}
}
@inproceedings{sokol2020explainability,
abstract = {Explanations in Machine Learning come in many forms, but a consensus regarding their desired properties is yet to emerge. In this paper we introduce a taxonomy and a set of descriptors that can be used to characterise and systematically assess explainable systems along five key dimensions: functional, operational, usability, safety and validation. In order to design a comprehensive and representative taxonomy and associated descriptors we surveyed the eXplainable Artificial Intelligence literature, extracting the criteria and desiderata that other authors have proposed or implicitly used in their research. The survey includes papers introducing new explainability algorithms to see what criteria are used to guide their development and how these algorithms are evaluated, as well as papers proposing such criteria from both computer science and social science perspectives. This novel framework allows to systematically compare and contrast explainability approaches, not just to better understand their capabilities but also to identify discrepancies between their theoretical qualities and properties of their implementations. We developed an operationalisation of the framework in the form of Explainability Fact Sheets, which enable researchers and practitioners alike to quickly grasp capabilities and limitations of a particular explainable method. When used as a Work Sheet, our taxonomy can guide the development of new explainability approaches by aiding in their critical evaluation along the five proposed dimensions.},
annote = {Pending. Explainability survey, the authors also propose a taxonomy aimed at describe methods 
in five dimensions: functional, operational, usability, safety and validation.},
author = {Sokol, Kacper and Flach, Peter},
booktitle = {arXiv},
issn = {23318422},
keywords = {AI,Desiderata,Explainability,Fact Sheet,Interpretability,ML,Taxonomy,Transparency,Work Sheet},
pages = {56--67},
title = {{Explainability fact sheets: a framework for systematic assessment of explainable approaches}},
year = {2019}
}
@inproceedings{seeliger2019semantic,
abstract = {Due to their tremendous potential in predictive tasks, Machine Learning techniques such as Artificial Neural Networks have received great attention from both research and practice. However, often these models do not provide explainable outcomes which is a crucial requirement in many high stakes domains such as health care or transport. Regarding explainability, Semantic Web Technologies offer semantically interpretable tools which allow reasoning on knowledge bases. Hence, the question arises how Semantic Web Technologies and related concepts can facilitate explanations in Machine Learning systems. To address this topic, we present current approaches of combining Machine Learning with Semantic Web Technologies in the context of model explainability based on a systematic literature review. In doing so, we also highlight domains and applications driving the research field and discuss the ways in which explanations are given to the user. Drawing upon these insights, we suggest directions for further research on combining Semantic Web Technologies with Machine Learning.},
annote = {Survey on KAs and Ontologies applications to XAI.},
author = {Seeliger, Arne and Pfaff, Matthias and Krcmar, Helmut},
booktitle = {CEUR Workshop Proceedings},
issn = {16130073},
keywords = {Explainability,Machine learning,Semantic web technologies,XAI},
pages = {30--45},
title = {{Semantic web technologies for explainable machine learning models: A literature review}},
volume = {2465},
year = {2019}
}
@article{futia2020integration,
abstract = {Deep learning models contributed to reaching unprecedented results in prediction and classification tasks of Artificial Intelligence (AI) systems. However, alongside this notable progress, they do not provide human-understandable insights on how a specific result was achieved. In contexts where the impact of AI on human life is relevant (e.g., recruitment tools, medical diagnoses, etc.), explainability is not only a desirable property, but it is-or, in some cases, it will be soon-a legal requirement. Most of the available approaches to implement eXplainable Artificial Intelligence (XAI) focus on technical solutions usable only by experts able to manipulate the recursive mathematical functions in deep learning algorithms. A complementary approach is represented by symbolic AI, where symbols are elements of a lingua franca between humans and deep learning. In this context, Knowledge Graphs (KGs) and their underlying semantic technologies are the modern implementation of symbolic AI-while being less flexible and robust to noise compared to deep learning models, KGs are natively developed to be explainable. In this paper, we review the main XAI approaches existing in the literature, underlying their strengths and limitations, and we propose neural-symbolic integration as a cornerstone to design an AI which is closer to non-insiders comprehension. Within such a general direction, we identify three specific challenges for future research-knowledge matching, cross-disciplinary explanations and interactive explanations.},
author = {Futia, Giuseppe and Vetr{\`{o}}, Antonio},
doi = {10.3390/info11020122},
issn = {20782489},
journal = {Information (Switzerland)},
keywords = {Deep learning,EXplainable artificial intelligence,Knowledge graphs},
number = {2},
pages = {122},
publisher = {Multidisciplinary Digital Publishing Institute},
title = {{On the integration of knowledge graphs into deep learning models for a more comprehensible AI-Three challenges for future research}},
volume = {11},
year = {2020}
}
@article{buhmann2016dl,
abstract = {In this system paper, we describe the DL-Learner framework, which supports supervised machine learning using OWL and RDF for background knowledge representation. It can be beneficial in various data and schema analysis tasks with applications in different standard machine learning scenarios, e.g.Â in the life sciences, as well as Semantic Web specific applications such as ontology learning and enrichment. Since its creation in 2007, it has become the main OWL and RDF-based software framework for supervised structured machine learning and includes several algorithm implementations, usage examples and has applications building on top of the framework. The article gives an overview of the framework with a focus on algorithms and use cases.},
author = {B{\"{u}}hmann, Lorenz and Lehmann, Jens and Westphal, Patrick},
doi = {10.1016/j.websem.2016.06.001},
issn = {15708268},
journal = {Journal of Web Semantics},
keywords = {Machine learning,OWL,RDF,Semantic Web,Supervised learning,System description},
pages = {15--24},
publisher = {Elsevier},
title = {{DL-Learnerâ€”A framework for inductive learning on the Semantic Web}},
volume = {39},
year = {2016}
}
@article{sarker2017explaining,
abstract = {The ever increasing prevalence of publicly available structured data on the World Wide Web enables new applications in a variety of domains. In this paper, we provide a conceptual approach that leverages such data in order to explain the input-output behavior of trained artificial neural networks. We apply existing Semantic Web technologies in order to provide an experimental proof of concept.},
archivePrefix = {arXiv},
arxivId = {1710.04324},
author = {Sarker, Md Kamruzzaman and Xie, Ning and Doran, Derek and Raymer, Michael and Hitzler, Pascal},
eprint = {1710.04324},
issn = {23318422},
journal = {arXiv},
title = {{Explaining trained neural networks with semantic web technologies: First steps}},
year = {2017}
}
@inproceedings{choi2017gram,
abstract = {Deep learning methods exhibit promising performance for predictive modeling in healthcare, but two important challenges remain: â€¢ Data insufficiency: Often in healthcare predictive modeling, the sample size is insufficient for deep learning methods to achieve satisfactory results. â€¢ Interpretation: The representations learned by deep learning methods should align with medical knowledge. To address these challenges, we propose GRaph-based Attention Model (GRAM) that supplements electronic health records (EHR) with hierarchical information inherent to medical ontologies. Based on the data volume and the ontology structure, GRAM represents a medical concept as a combination of its ancestors in the ontology via an attention mechanism. We compared predictive performance (i.e. accuracy, data needs, interpretability) of GRAM to various methods including the recurrent neural network (RNN) in two sequential diagnoses prediction tasks and one heart failure prediction task. Compared to the basic RNN, GRAM achieved 10{\%} higher accuracy for predicting diseases rarely observed in the training data and 3{\%} improved area under the ROC curve for predicting heart failure using an order of magnitude less training data. Additionally, unlike other methods, the medical concept representations learned by GRAM are well aligned with the medical ontology. Finally, GRAM exhibits intuitive attention behaviors by adaptively generalizing to higher level concepts when facing data insufficiency at the lower level concepts.},
archivePrefix = {arXiv},
arxivId = {1611.07012},
author = {Choi, Edward and Bahadori, Mohammad Taha and Song, Le and Stewart, Walter F. and Sun, Jimeng},
booktitle = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
doi = {10.1145/3097983.3098126},
eprint = {1611.07012},
isbn = {9781450348874},
keywords = {Attention model,Electronic health records,Graph,Predictive healthcare},
pages = {787--795},
title = {{GRAM: Graph-based attention model for healthcare representation learning}},
volume = {Part F1296},
year = {2017}
}
@article{healthcare2018clinical,
abstract = {The Kids' Inpatient Database (KID) is part of the Healthcare Cost and Utilization Project (HCUP), sponsored by the Agency for Healthcare Research and Quality (AHRQ), formerly the Agency for Health Care Policy and Research. The KID is the only dataset on hospital use, outcomes, and charges designed to study children's use of hospital services in the United States. The KID is a sample of discharges from all community, non-rehabilitation hospitals in States participating in HCUP. The target universe includes pediatric discharges from community, non-rehabilitation hospitals in the United States. Pediatric discharges are defined as all discharges where the patient was age 20 or less at admission. See Table 1 in Appendix I for a list of the statewide data organizations participating in the KID. The number of sample hospitals and discharges by State and year are available in Table 2 in Appendix I. The KID contains charge information on all patients, regardless of payer, including persons covered by private insurance, Medicaid, Medicare, and the uninsured. The KID's large sample size enables analyses of rare conditions, such as congenital anomalies and uncommon treatments, such as organ transplantation. It can be used to study a wide range of topics including the economic burden of pediatric conditions, access to services, quality of care and patient safety, and the impact of health policy changes. Inpatient stay records in the KID include clinical and resource use information typically available from discharge abstracts. Discharge weights are provided for calculating national estimates. The KID can be linked to hospital-level data from the American Hospital Association's Annual Survey Database (Health Forum, LLC {\textcopyright} 2010) and county-level data from the Bureau of Health Professions' Area Resource File, except in those States that do not allow the release of hospital identifiers. The KID is available every three years beginning with 1997. Periodically, new data elements are added to the KID and some are dropped; see Appendix III for a summary of data elements and when they are effective. Access to the KID is open to users who sign Data Use Agreements. Uses are limited to research and aggregate statistical reporting.HCUP KID (02/19/2013) 6 Introduction For more information on the KID, visit the AHRQ-sponsored HCUP User Support (HCUP-US) Website at http://www.hcup-us.ahrq.gov.},
author = {Elixhauser, a and Steiner, C and Palmer, L},
journal = {U.S. Agency for Healthcare Research and Quality},
number = {November 2013},
pages = {1--54},
title = {{Clinical Classifications Software (CCS), 2014}},
url = {http://www.hcup-us.ahrq.gov/toolssoftware/ccs/ccs.jsp},
volume = {27},
year = {2014}
}
@inproceedings{ghidini2019quantitative,
abstract = {Deep Learning models have recently achieved incredible performances in the Computer Vision field and are being deployed in an ever-growing range of real-life scenarios. Since they do not intrinsically provide insights of their inner decision processes, the field of eXplainable Artificial Intelligence emerged. Different XAI techniques have already been proposed, but the existing literature lacks methods to quantitatively compare different explanations, and in particular the semantic component is systematically overlooked. In this paper we introduce quantitative and ontology-based techniques and metrics in order to enrich and compare different explanations and XAI algorithms.},
author = {Ghidini, Valentina and Perotti, Alan and Schifanella, Rossano},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-030-37599-7_6},
isbn = {9783030375980},
issn = {16113349},
keywords = {Computer vision,Deep learning,Explainable artificial intelligence,Neural networks},
organization = {Springer},
pages = {58--70},
title = {{Quantitative and Ontology-Based Comparison of Explanations for Image Classification}},
volume = {11943 LNCS},
year = {2019}
}
@article{confalonieri2019trepan,
abstract = {Explainability in Artificial Intelligence has been revived as a topic of active research by the need of conveying safety and trust to users in the `how' and `why' of automated decision-making. Whilst a plethora of approaches have been developed for post-hoc explainability, only a few focus on how to use domain knowledge, and how this influences the understandability of global explanations from the users' perspective. In this paper, we show how ontologies help the understandability of global post-hoc explanations, presented in the form of symbolic models. In particular, we build on Trepan, an algorithm that explains artificial neural networks by means of decision trees, and we extend it to include ontologies modeling domain knowledge in the process of generating explanations. We present the results of a user study that measures the understandability of decision trees using a syntactic complexity measure, and through time and accuracy of responses as well as reported user confidence and understandability. The user study considers domains where explanations are critical, namely, in finance and medicine. The results show that decision trees generated with our algorithm, taking into account domain knowledge, are more understandable than those generated by standard Trepan without the use of ontologies.},
archivePrefix = {arXiv},
arxivId = {1906.08362},
author = {Confalonieri, Roberto and Weyde, Tillman and Besold, Tarek R. and Mart{\'{i}}n, Ferm{\'{i}}n Moscoso del Prado},
eprint = {1906.08362},
journal = {arXiv preprint arXiv:1906.08362},
title = {{Trepan Reloaded: A Knowledge-driven Approach to Explaining Artificial Neural Networks}},
url = {http://arxiv.org/abs/1906.08362},
year = {2019}
}
@inproceedings{panigutti2020doctor,
abstract = {Several recent advancements in Machine Learning involve black-box models: algorithms that do not provide human-understandable explanations in support of their decisions. This limitation hampers the fairness, accountability and transparency of these models; the field of eXplainable Artificial Intelligence (XAI) tries to solve this problem providing human-understandable explanations for black-box models. However, healthcare datasets (and the related learning tasks) often present peculiar features, such as sequential data, multi-label predictions, and links to structured background knowledge. In this paper, we introduce Doctor XAI, a model-agnostic explainability technique able to deal with multi-labeled, sequential, ontology-linked data. We focus on explaining Doctor AI, a multi-label classifier which takes as input the clinical history of a patient in order to predict the next visit. Furthermore, we show how exploiting the temporal dimension in the data and the domain knowledge encoded in the medical ontology improves the quality of the mined explanations.},
author = {Panigutti, Cecilia and Perotti, Alan and Pedreschi, Dino},
booktitle = {FAT* 2020 - Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
doi = {10.1145/3351095.3372855},
isbn = {9781450369367},
keywords = {Explainable artificial intelligence,Healthcare data,Machine learning},
pages = {629--639},
title = {{Doctor XAI An ontology-based approach to black-box sequential data classification explanations}},
year = {2020}
}
@article{lai2020ontology,
abstract = {In this paper, we introduce a novel interpreting framework that learns an interpretable model based on an ontology-based sampling technique to explain agnostic prediction models. Different from existing approaches, our algorithm considers contextual correlation among words, described in domain knowledge ontologies, to generate semantic explanations. To narrow down the search space for explanations, which is a major problem of long and complicated text data, we design a learnable anchor algorithm, to better extract explanations locally. A set of regulations is further introduced, regarding combining learned interpretable representations with anchors to generate comprehensible semantic explanations. An extensive experiment conducted on two real-world datasets shows that our approach generates more precise and insightful explanations compared with baseline approaches.},
archivePrefix = {arXiv},
arxivId = {2004.00204},
author = {Lai, Thi Kim Phung and Phan, Nhat Hai and Hu, Han and Badeti, Anuja and Newman, David and Dou, Dejing},
eprint = {2004.00204},
issn = {23318422},
journal = {arXiv},
keywords = {Anchor,Information extraction,Interpretable machine learning,Natural language processing,Ontology},
title = {{Ontology-based interpretable machine learning for textual data}},
year = {2020}
}
@article{miller1995wordnet,
abstract = {This database links English nouns, verbs, adjectives, and adverbs to sets of synonyms that are in turn linked through semantic relations that determine word definitions. {\textcopyright} 1995, ACM. All rights reserved.},
annote = {Wordnet ontology paper},
author = {Miller, George A.},
doi = {10.1145/219717.219748},
issn = {15577317},
journal = {Communications of the ACM},
number = {11},
pages = {39--41},
publisher = {ACM New York, NY, USA},
title = {{WordNet: A Lexical Database for English}},
volume = {38},
year = {1995}
}
@inproceedings{stearns2001snomed,
abstract = {Two large health care reference terminologies, SNOMED RT and Clinical Terms Version 3 , are in the process of being merged to form a comprehensive new work referred to as SNOMED Clinical Terms. The College of American Pathologists and the United Kingdom s National Health Service have entered into a collaborative agreement to develop this new work. Both organizations have extensive terminology development and maintenance experience. This paper discusses the process and status of SNOMED CT development and how the resources and expertise of both organizations are being used to develop this new terminological resource. The preliminary results of the merger process, including mapping, the merger of upper levels of each hierarchy, and attribute harmonization are also discussed.},
author = {Stearns, M. Q. and Price, C. and Spackman, K. A. and Wang, A. Y.},
booktitle = {Proceedings / AMIA ... Annual Symposium. AMIA Symposium},
issn = {1531605X},
organization = {American Medical Informatics Association},
pages = {662--666},
pmid = {11825268},
title = {{SNOMED clinical terms: overview of the development process and project status.}},
year = {2001}
}
