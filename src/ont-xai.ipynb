{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "source": [
    "### Distances"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from similarity.weighted_levenshtein import WeightedLevenshtein\n",
    "from similarity.weighted_levenshtein import CharacterSubstitutionInterface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wup_seq(c2c, v1, v2):\n",
    "    all_icd9 = sorted(list(set(v1 + v2)))\n",
    "    symbols = [chr(i) for i in range(len(all_icd9))]  # fix this\n",
    "    coded_all = [symbols[i] for i in range(len(all_icd9))]\n",
    "    coded_v1 = ''.join([coded_all[all_icd9.index(c)] for c in sorted(v1)])\n",
    "    coded_v2 = ''.join([coded_all[all_icd9.index(c)] for c in sorted(v2)])\n",
    "    encoder = {k: v for (k, v) in zip(all_icd9, coded_all)}\n",
    "    decoder = {v: k for (k, v) in zip(all_icd9, coded_all)}\n",
    "\n",
    "    encoded_c2c = {}\n",
    "    for icd9_1 in v1:\n",
    "        for icd9_2 in v2:\n",
    "            encoded_c2c[(encoder[icd9_1], encoder[icd9_2])] = 1 - c2c[(icd9_1, icd9_2)]\n",
    "\n",
    "    class CharacterSubstitution(CharacterSubstitutionInterface):\n",
    "        def cost(self, c0, c1):\n",
    "            return encoded_c2c[(c0, c1)]\n",
    "\n",
    "    weighted_levenshtein = WeightedLevenshtein(CharacterSubstitution())\n",
    "    return weighted_levenshtein.distance(coded_v1, coded_v2)\n",
    "\n",
    "def wup_points(c2c, s, t):\n",
    "    n = len(s)\n",
    "    m = len(t)\n",
    "    dtw = np.full((n, m), 10000, dtype=np.float64)\n",
    "    dtw[0, 0] = 0\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            cost = np.round(wup_seq(c2c, s[i], t[j]), 3)\n",
    "            in_cost = dtw[i - 1, j] if i > 0 else 0\n",
    "            del_cost = dtw[i, j - 1] if j > 0 else 0\n",
    "            edit_cost = dtw[i - 1, j - 1] if i > 0 and j > 0 else 0\n",
    "            dtw[i, j] = cost + min(in_cost, del_cost, edit_cost)\n",
    "    return dtw[-1][-1]\n",
    "\n",
    "def wup_concepts(a1, a2):\n",
    "    \"\"\"\n",
    "    This function calculates the WUP distance between concept C1 and concept C2 in the ontology:\n",
    "\n",
    "    WUP(C1,C2) = (2*depth_LCA)/(N_1+N_2+depth_LCA)\n",
    "\n",
    "    where depth_LCA is the depth of the last common ancestor (LCA) of concept1 and concept2\n",
    "    N_1 is the number of nodes from LCA to concept C1\n",
    "    N_2 is the number of nodes from LCA to concept C2\n",
    "\n",
    "    :param a1: path from ROOT to the concept C1\n",
    "    :param a2: path from ROOT to the concept C2\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # common substring length -> depth of last common ancestor (LCA)\n",
    "    # a1, a2 are PATHS FROM ROOT\n",
    "\n",
    "    depth_LCA = 0\n",
    "    while True:\n",
    "        if depth_LCA >= len(a1) or depth_LCA >= len(a2) or a1[depth_LCA] != a2[depth_LCA]:\n",
    "            break\n",
    "        depth_LCA += 1\n",
    "\n",
    "    dr = depth_LCA - 1\n",
    "    da = len(a1) - depth_LCA\n",
    "    db = len(a2) - depth_LCA\n",
    "\n",
    "    return ((2 * dr) / (da + db + 2 * dr))"
   ]
  },
  {
   "source": [
    "### Utils"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_save(obj, name):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_CSV_ontology(ontology_path_file):\n",
    "    \"\"\"\n",
    "    :param ontology_path_file: the path to the ontology file with the name  of the CSV file\n",
    "    e.g. \"/path/to/file/ICD9_ontology.csv\"\n",
    "    :return: a networkx Directed Graph\n",
    "    \"\"\"\n",
    "    ont_tree = nx.DiGraph()\n",
    "    nodes = set()\n",
    "    line_count = 0\n",
    "    with open(ontology_path_file) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        for row in csv_reader:\n",
    "            line_count += 1\n",
    "            ont_tree.add_edge(row[1], row[0])\n",
    "            nodes.add(row[1])\n",
    "            nodes.add(row[0])\n",
    "        print(f'The ontology has {line_count} \"is-a\" links.')\n",
    "        print(f'The ontology has {len(nodes)} nodes')\n",
    "\n",
    "    return ont_tree\n",
    "\n",
    "def subset_ontology_from_data(ont_tree, dataset_sequences):\n",
    "    \"\"\"\n",
    "    This function selects the subset of concepts from the original ontology that also appears in the data\n",
    "    :param ont_tree: nx.DiGraph, original ontology\n",
    "    :param dataset_sequences: list of lists of lists, the sequences of the dataset\n",
    "    :return: nx.DiGraph\n",
    "    \"\"\"\n",
    "\n",
    "    data_concepts = sorted(list(set([a for b in [c for d in dataset_sequences for c in d] for a in b])))\n",
    "    data_nodes = []\n",
    "\n",
    "    for l in data_concepts:\n",
    "        data_nodes += nx.shortest_path(ont_tree, 'ROOT', l)\n",
    "    data_nodes = sorted(list(set(data_nodes)))\n",
    "\n",
    "    # PARENT->CHILD\n",
    "    subset_ontology = nx.DiGraph()\n",
    "    for child in data_nodes:\n",
    "        if child == 'ROOT':\n",
    "            continue\n",
    "        parent = list(ont_tree.in_edges(child))[0][0]\n",
    "        subset_ontology.add_edge(parent, child)\n",
    "\n",
    "    return subset_ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_c2c(ont_tree, dataset, output_path, verbose=True):\n",
    "    \"\"\"\n",
    "    This function computes and save a python dictionary containing all pairwise distances between the concepts of the ontology present in the dataset used to train and test the blackbox.\n",
    "\n",
    "    :param ont_tree: nx.DiGraph, is the ontology\n",
    "    :param dataset: list of lists of lists, is the dataset used to train and test the blackbox\n",
    "    :param output_path: str, is the path where you want to save the output\n",
    "    :return: python dictionary, with key the tuple of concepts and value the pairwise distances between the concepts\n",
    "    \"\"\"\n",
    "\n",
    "    data_concepts = sorted(list(set([a for b in [c for d in dataset for c in d] for a in b])))\n",
    "    subset_ontology = subset_ontology_from_data(ont_tree, dataset)\n",
    "    paths_from_ROOT = [nx.shortest_path(subset_ontology, 'ROOT', c) for c in data_concepts]\n",
    "    n_paths = len(paths_from_ROOT)\n",
    "\n",
    "    c2c_dict = {}\n",
    "\n",
    "    for i, v1 in enumerate(paths_from_ROOT):\n",
    "        if verbose:\n",
    "            if i % 100 == 0:\n",
    "                print(f'{i} concepts elaborated out of {n_paths}')\n",
    "        for v2 in paths_from_ROOT:\n",
    "            c2c_dict[(v1[-1], v2[-1])] = wup_concepts(v1, v2)\n",
    "    if verbose:\n",
    "        print(f'{i} concepts elaborated')\n",
    "    pickle_save(c2c_dict, f'{output_path}/c2c_dist')\n",
    "\n",
    "    return c2c_dict"
   ]
  },
  {
   "source": [
    "### Real neighbors identification"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these are the pickle dictionaries containing the descriptions of the codes (ICD-9 and CCS codes)\n",
    "ICD9_description_dict = pickle.load(open('../ds/ICD9_description_dict.pkl', 'rb'))\n",
    "CCS_description_dict = pickle.load(open('../ds/CCS_description_dict.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.load('../ds/preprocessing_doctorai/mimic_sequences.npy',allow_pickle=True)\n",
    "datapoint = dataset[0]\n",
    "ontology_path = '../ds/ICD9_ontology.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The ontology has 22535 \"is-a\" links.\nThe ontology has 22536 nodes\n"
     ]
    }
   ],
   "source": [
    "ont_tree = read_CSV_ontology(ontology_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 concepts elaborated out of 259\n100 concepts elaborated out of 259\n200 concepts elaborated out of 259\n258 concepts elaborated\n"
     ]
    }
   ],
   "source": [
    "c2c_distance_matrix = compute_c2c(ont_tree, dataset, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_neighbors(datapoint, dataset, c2c_distance_matrix, n_first_neighbors = 10):\n",
    "        \"\"\"\n",
    "        This function finds the first k (n_first_neighbors) closest neighbors of the analyzed patient (patient_sequence)\n",
    "        in the dataset (dataset_sequences). It does so by using the ontology.\n",
    "        :return: list of lists of lists, closest neighbours patients' sequences including the patient under analysis\n",
    "        \"\"\"\n",
    "\n",
    "        if n_first_neighbors<0:\n",
    "            print('n_first_neighbors should be a positive integer or 0')\n",
    "        if n_first_neighbors==0:\n",
    "            print('Perturbing only the patients to be explained')\n",
    "            real_neighs = [datapoint]\n",
    "        else:\n",
    "            #calculate the k closest neighbors in the dataset\n",
    "            c2c_dict = c2c_distance_matrix\n",
    "            points_dist = {}\n",
    "            \n",
    "            for i, point in enumerate(dataset):\n",
    "                points_dist[i] = wup_points(c2c_dict, datapoint, point)\n",
    "            \n",
    "            closest_neigh_indexes = list(\n",
    "                    {k: v for k, v in \n",
    "                    sorted(points_dist.items(), key=lambda item: item[1])\n",
    "                    }.keys())[:n_first_neighbors+1]\n",
    "            real_neighs = [dataset[idx] for idx in closest_neigh_indexes]\n",
    "\n",
    "        return real_neighs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['571.5',\n",
       "  '584.9',\n",
       "  '070.70',\n",
       "  '280.0',\n",
       "  '287.5',\n",
       "  '789.5',\n",
       "  '456.20',\n",
       "  '572.3',\n",
       "  '401.9'],\n",
       " ['571.5',\n",
       "  '452',\n",
       "  '070.70',\n",
       "  '280.0',\n",
       "  '456.20',\n",
       "  '038.9',\n",
       "  '995.92',\n",
       "  '785.52',\n",
       "  '518.81',\n",
       "  '572.4',\n",
       "  '584.9',\n",
       "  '585.9',\n",
       "  '789.5',\n",
       "  '572.3']]"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "find_closest_neighbors(datapoint, dataset, c2c_distance_matrix, 20)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}